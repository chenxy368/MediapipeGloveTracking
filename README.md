# MediapipeGloveTracking
This project proposes a vision-based glove tracking and gesture recognition system. Our approach generalizes hand-tracking solutions to glove tracking using a deep learning-based preprocessing method and establishes a highly robust gesture recognition module based on the skeleton output. The system comprises three main components. The first component is a preprocessing method that utilizes the YOLOv7 instance segmentation model to obtain a mask of the glove. Once the mask is acquired, we transform the glove's color to a skin-like color in the CIELAB color space. This preprocessing step enables our system to adapt to various glove colors, patterns, materials, and styles while maintaining compatibility with existing hand-tracking solutions. With the preprocessed image, the second component employs Google's open-source MediaPipe hand-tracking solution to extract the glove's skeleton representation. This step allows our system to capture the underlying structure of the glove and its movements, which is crucial for the subsequent gesture recognition stage. The third component focuses on gesture recognition. Leveraging the extracted skeleton representation, we employ a heuristic approach and deep learning models to recognize static gestures for dynamic gesture recognition. To enhance the generalization capabilities of our preprocessing method, we develop a module that automatically generates segmentation annotations and training samples for fine-tuning the YOLOv7 model's weights. This module further improves the system's ability to handle the vast array of gloves.
## Poster Overview
<p align="center">
   <img src="readme/xinyang_chen.jpg">
</p>
